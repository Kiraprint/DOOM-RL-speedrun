{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436e36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e0732a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e887cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CustomYOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self, images_dir, sequence_length=5, skip=2, img_size=640, transform=None\n",
    "    ):\n",
    "        self.images_dir = images_dir\n",
    "        self.img_files = sorted(\n",
    "            [f for f in os.listdir(images_dir) if f.endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.transform = (\n",
    "            transform\n",
    "            if transform\n",
    "            else transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.df = pd.DataFrame(self.img_files, columns=[\"img_name\"])\n",
    "        # Split image file names into 'map', 'batch_id', 'img_id' columns\n",
    "        self.df[[\"map\", \"batch_id\", \"img_id\"]] = self.df[\"img_name\"].str.extract(\n",
    "            r\"(.+?)_(\\d+)_manual_frame(\\d+)\\.png\"\n",
    "        )\n",
    "        self.df[\"img_id\"] = self.df[\"img_id\"].astype(int)\n",
    "        self.df[\"batch_id\"] = self.df[\"batch_id\"].astype(int)\n",
    "        self.df[\"id\"] = self.df[\"batch_id\"] * 10 + self.df[\"img_id\"]\n",
    "        self.sequences = []\n",
    "        self.k = sequence_length\n",
    "        self.skip = skip\n",
    "        for _, group in self.df.groupby(\"map\"):\n",
    "            group_sorted = group.sort_values(by=[\"batch_id\", \"img_id\"])\n",
    "            img_ids = group_sorted[\"img_id\"].values\n",
    "            indices = group_sorted.index.values\n",
    "            max_start = len(img_ids) - (self.k - 1) * skip\n",
    "            for i in range(max_start):\n",
    "                seq_indices = indices[i : i + self.k * skip : skip]\n",
    "                if len(seq_indices) == self.k:\n",
    "                    self.sequences.append(\n",
    "                        group[\"img_name\"][seq_indices].values.tolist()\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_names = self.sequences[idx]\n",
    "        images = []\n",
    "        for img_name in img_names:\n",
    "            image = self.process_img(img_name)\n",
    "            images.append(image)\n",
    "        images = torch.stack(images)\n",
    "        return images\n",
    "\n",
    "    def process_img(self, img_name: str):\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e0492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"../datasets/DOOM_yolo_seg/\"\n",
    "dataset = CustomYOLODataset(images_dir=DATASET + \"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f612023",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf8ca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(dataloader))\n",
    "len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66a369d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 3, 640, 640])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5588629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOFeatureExtractor(nn.Module):\n",
    "    def __init__(self, yolo_model, layer_idx=17):\n",
    "        super().__init__()\n",
    "        self.model = yolo_model\n",
    "        self.layer_idx = layer_idx\n",
    "        self.features = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get batch shape dynamically\n",
    "        batch_size, seq_len, channels, height, width = x.shape\n",
    "        # Clear previous features\n",
    "        self.features = None\n",
    "        # Reshape for YOLO model\n",
    "        x_reshaped = x.view(-1, channels, height, width)\n",
    "        results = self.model.predict(x_reshaped, embed=[self.layer_idx], verbose=False)\n",
    "        return torch.concat(results).view(batch_size, seq_len, -1)      \n",
    "\n",
    "# Usage\n",
    "feature_extractor = YOLOFeatureExtractor(model, layer_idx=22)\n",
    "embeddings = feature_extractor(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ed97d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 256])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape  # Should be [batch_size, seq_len, emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78a1606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, emb_dim = embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "145905eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernEmbeddingLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        bidirectional=False,\n",
    "        use_attention=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate directions\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        # LSTM layer with dropout between layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * self.directions)\n",
    "        # Attention mechanism\n",
    "        if use_attention:\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * self.directions, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "            )\n",
    "        # Output projection to match embedding dimension\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * self.directions, hidden_dim * 2),\n",
    "            nn.GELU(),  # Modern activation function\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, embedding_dim),\n",
    "        )\n",
    "        # Initialize weights for better convergence\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                if \"lstm\" in name:\n",
    "                    nn.init.orthogonal_(param)  # Orthogonal initialization for RNNs\n",
    "                elif len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(\n",
    "                        param\n",
    "                    )  # Xavier for linear layers (2D+ weights)\n",
    "                else:\n",
    "                    nn.init.normal_(\n",
    "                        param, mean=0.0, std=0.02\n",
    "                    )  # Normal init for 1D weights\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)  # Initialize biases to zero\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, embedding_dim]\n",
    "            return_attention: Whether to return attention weights\n",
    "\n",
    "        Returns:\n",
    "            Predicted next embedding of shape [batch_size, embedding_dim]\n",
    "            Optionally, attention weights if return_attention=True\n",
    "        \"\"\"\n",
    "        # x shape: [B, T, embedding_dim]\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "\n",
    "        # Run through LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out shape: [B, T, hidden_dim * directions]\n",
    "\n",
    "        # Apply layer normalization\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        if self.use_attention:\n",
    "            # Calculate attention scores\n",
    "            attention_scores = self.attention(lstm_out).squeeze(-1)  # [B, T]\n",
    "            attention_weights = torch.softmax(attention_scores, dim=1)  # [B, T]\n",
    "\n",
    "            # Apply attention to get context vector\n",
    "            attention_weights = attention_weights.unsqueeze(-1)  # [B, T, 1]\n",
    "            context = torch.sum(\n",
    "                attention_weights * lstm_out, dim=1\n",
    "            )  # [B, hidden_dim * directions]\n",
    "        else:\n",
    "            # Just use the last output\n",
    "            context = lstm_out[:, -1]  # [B, hidden_dim * directions]\n",
    "\n",
    "        # Project to embedding dimension\n",
    "        next_emb = self.proj(context)  # [B, embedding_dim]\n",
    "\n",
    "        if return_attention and self.use_attention:\n",
    "            return next_emb, attention_weights.squeeze(-1)\n",
    "        return next_emb\n",
    "\n",
    "    def predict_sequence(self, initial_sequence, steps=5):\n",
    "        \"\"\"\n",
    "        Generate a sequence of future embeddings\n",
    "\n",
    "        Args:\n",
    "            initial_sequence: Initial sequence of embeddings [B, T, embedding_dim]\n",
    "            steps: Number of future steps to predict\n",
    "\n",
    "        Returns:\n",
    "            Sequence of predicted embeddings [B, steps, embedding_dim]\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = initial_sequence.shape[0]\n",
    "        emb_dim = initial_sequence.shape[2]\n",
    "\n",
    "        # Start with the initial sequence\n",
    "        current_seq = initial_sequence\n",
    "        predictions = torch.zeros(batch_size, steps, emb_dim, device=device)\n",
    "\n",
    "        # Autoregressive prediction\n",
    "        for i in range(steps):\n",
    "            # Predict next embedding\n",
    "            next_emb = self(current_seq)\n",
    "            predictions[:, i] = next_emb\n",
    "\n",
    "            # Update sequence for next prediction (drop oldest, add newest)\n",
    "            if i < steps - 1:  # No need to update for the last step\n",
    "                current_seq = torch.cat(\n",
    "                    [current_seq[:, 1:], next_emb.unsqueeze(1)], dim=1\n",
    "                )\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d5f92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernEmbeddingLSTM(\n",
       "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (proj): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "lstm_model = ModernEmbeddingLSTM(\n",
    "    embedding_dim=emb_dim, hidden_dim=512, num_layers=2, dropout=0.1, use_attention=True\n",
    ")\n",
    "lstm_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7447388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next embedding shape: torch.Size([8, 256])\n",
      "Attention weights shape: torch.Size([8, 5])\n",
      "Future sequence shape: torch.Size([8, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "# Predict next embedding\n",
    "with torch.inference_mode():\n",
    "    next_embedding = lstm_model(embeddings)\n",
    "    print(f\"Next embedding shape: {next_embedding.shape}\")  # [batch_size, emb_dim]\n",
    "\n",
    "    # With attention weights\n",
    "    next_embedding, attention = lstm_model(embeddings, return_attention=True)\n",
    "    print(f\"Attention weights shape: {attention.shape}\")  # [batch_size, seq_len]\n",
    "\n",
    "    # Generate a sequence of future embeddings\n",
    "    future_sequence = lstm_model.predict_sequence(embeddings, steps=1)\n",
    "    print(\n",
    "        f\"Future sequence shape: {future_sequence.shape}\"\n",
    "    )  # [batch_size, 10, emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8366483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/validation split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define split ratio\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b5e4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_lstm(\n",
    "    feature_extractor,\n",
    "    lstm_model,\n",
    "    dataloader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    # Move models to device\n",
    "    feature_extractor.to(device)\n",
    "    lstm_model.to(device)\n",
    "\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        lstm_model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # Training history\n",
    "    history = {\"train_loss\": []}\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        lstm_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Extract embeddings from the entire sequence\n",
    "            embeddings = feature_extractor(batch)  # [B, T, emb_dim]\n",
    "            embeddings = embeddings.to(device)\n",
    "\n",
    "            # Use first N-1 embeddings to predict the Nth embedding\n",
    "            input_embeddings = embeddings[:, :-1, :]  # [B, T-1, emb_dim]\n",
    "            target_embedding = embeddings[:, -1, :]  # [B, emb_dim]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            predicted_embedding = lstm_model(input_embeddings)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_embedding, target_embedding)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.6f}\"\n",
    "                )\n",
    "\n",
    "        # End of epoch\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} completed. Avg Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}\"\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": lstm_model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": best_loss,\n",
    "                },\n",
    "                \"best_lstm_model.pt\",\n",
    "            )\n",
    "            print(f\"Model saved with loss: {best_loss:.6f}\")\n",
    "\n",
    "    return lstm_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372b611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Batch 0/476, Loss: 0.052686\n",
      "Epoch 1/50, Batch 10/476, Loss: 0.099270\n",
      "Epoch 1/50, Batch 20/476, Loss: 26.343069\n",
      "Epoch 1/50, Batch 30/476, Loss: 0.970966\n",
      "Epoch 1/50, Batch 40/476, Loss: 0.467288\n",
      "Epoch 1/50, Batch 50/476, Loss: 0.076698\n",
      "Epoch 1/50, Batch 60/476, Loss: 0.496063\n",
      "Epoch 1/50, Batch 70/476, Loss: 3.362388\n",
      "Epoch 1/50, Batch 80/476, Loss: 1.755894\n",
      "Epoch 1/50, Batch 90/476, Loss: 190.672043\n",
      "Epoch 1/50, Batch 100/476, Loss: 8.184610\n",
      "Epoch 1/50, Batch 110/476, Loss: 58.297813\n",
      "Epoch 1/50, Batch 120/476, Loss: 0.990378\n",
      "Epoch 1/50, Batch 130/476, Loss: 44.822739\n",
      "Epoch 1/50, Batch 140/476, Loss: 2.382850\n",
      "Epoch 1/50, Batch 150/476, Loss: 375.511841\n",
      "Epoch 1/50, Batch 160/476, Loss: 1.044628\n",
      "Epoch 1/50, Batch 170/476, Loss: 0.223539\n",
      "Epoch 1/50, Batch 180/476, Loss: 0.427817\n",
      "Epoch 1/50, Batch 190/476, Loss: 0.209548\n",
      "Epoch 1/50, Batch 200/476, Loss: 113.032585\n",
      "Epoch 1/50, Batch 210/476, Loss: 0.383290\n",
      "Epoch 1/50, Batch 220/476, Loss: 1.896260\n",
      "Epoch 1/50, Batch 230/476, Loss: 15.356440\n",
      "Epoch 1/50, Batch 240/476, Loss: 0.815443\n",
      "Epoch 1/50, Batch 250/476, Loss: 57.044109\n",
      "Epoch 1/50, Batch 260/476, Loss: 1.533446\n",
      "Epoch 1/50, Batch 270/476, Loss: 4.931336\n",
      "Epoch 1/50, Batch 280/476, Loss: 149.380646\n",
      "Epoch 1/50, Batch 290/476, Loss: 0.187586\n",
      "Epoch 1/50, Batch 300/476, Loss: 2.241108\n",
      "Epoch 1/50, Batch 310/476, Loss: 0.420073\n",
      "Epoch 1/50, Batch 320/476, Loss: 135.891403\n",
      "Epoch 1/50, Batch 330/476, Loss: 0.051379\n",
      "Epoch 1/50, Batch 340/476, Loss: 100.185638\n",
      "Epoch 1/50, Batch 350/476, Loss: 0.147896\n",
      "Epoch 1/50, Batch 360/476, Loss: 1.021466\n",
      "Epoch 1/50, Batch 370/476, Loss: 252.839310\n",
      "Epoch 1/50, Batch 380/476, Loss: 28.004074\n",
      "Epoch 1/50, Batch 390/476, Loss: 1.977513\n",
      "Epoch 1/50, Batch 400/476, Loss: 0.020519\n",
      "Epoch 1/50, Batch 410/476, Loss: 6.192109\n",
      "Epoch 1/50, Batch 420/476, Loss: 2.980515\n",
      "Epoch 1/50, Batch 430/476, Loss: 229.793671\n",
      "Epoch 1/50, Batch 440/476, Loss: 1.354266\n",
      "Epoch 1/50, Batch 450/476, Loss: 307.183258\n",
      "Epoch 1/50, Batch 460/476, Loss: 1.731088\n",
      "Epoch 1/50, Batch 470/476, Loss: 8.345860\n",
      "Epoch 1/50 completed. Avg Loss: 42.750035, LR: 0.000999\n",
      "Model saved with loss: 42.750035\n",
      "Epoch 2/50, Batch 0/476, Loss: 54.505302\n",
      "Epoch 2/50, Batch 10/476, Loss: 8.295725\n",
      "Epoch 2/50, Batch 20/476, Loss: 0.254864\n",
      "Epoch 2/50, Batch 30/476, Loss: 0.082395\n",
      "Epoch 2/50, Batch 40/476, Loss: 0.166715\n",
      "Epoch 2/50, Batch 50/476, Loss: 79.682159\n",
      "Epoch 2/50, Batch 60/476, Loss: 0.097519\n",
      "Epoch 2/50, Batch 70/476, Loss: 612.497070\n",
      "Epoch 2/50, Batch 80/476, Loss: 0.630273\n",
      "Epoch 2/50, Batch 90/476, Loss: 0.110801\n",
      "Epoch 2/50, Batch 100/476, Loss: 0.219258\n",
      "Epoch 2/50, Batch 110/476, Loss: 245.262939\n",
      "Epoch 2/50, Batch 120/476, Loss: 0.469719\n",
      "Epoch 2/50, Batch 130/476, Loss: 2.203858\n",
      "Epoch 2/50, Batch 140/476, Loss: 63.945526\n",
      "Epoch 2/50, Batch 150/476, Loss: 0.265625\n",
      "Epoch 2/50, Batch 160/476, Loss: 25.710911\n",
      "Epoch 2/50, Batch 170/476, Loss: 0.118857\n",
      "Epoch 2/50, Batch 180/476, Loss: 20.395765\n",
      "Epoch 2/50, Batch 190/476, Loss: 0.636479\n",
      "Epoch 2/50, Batch 200/476, Loss: 2.207215\n",
      "Epoch 2/50, Batch 210/476, Loss: 9.492684\n",
      "Epoch 2/50, Batch 220/476, Loss: 0.665970\n",
      "Epoch 2/50, Batch 230/476, Loss: 23.619505\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)\n",
    "\n",
    "# Train the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "trained_model, history = train_embedding_lstm(\n",
    "    feature_extractor=feature_extractor,\n",
    "    lstm_model=lstm_model,\n",
    "    dataloader=train_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f91801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
