{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436e36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0732a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e887cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class CustomYOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self, images_dir, sequence_length=5, skip=2, img_size=640, transform=None\n",
    "    ):\n",
    "        self.images_dir = images_dir\n",
    "        self.img_files = sorted(\n",
    "            [f for f in os.listdir(images_dir) if f.endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.transform = (\n",
    "            transform\n",
    "            if transform\n",
    "            else transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.df = pd.DataFrame(self.img_files, columns=[\"img_name\"])\n",
    "        # Split image file names into 'map', 'batch_id', 'img_id' columns\n",
    "        self.df[[\"map\", \"batch_id\", \"img_id\"]] = self.df[\"img_name\"].str.extract(\n",
    "            r\"(.+?)_(\\d+)_manual_frame(\\d+)\\.png\"\n",
    "        )\n",
    "        self.df[\"img_id\"] = self.df[\"img_id\"].astype(int)\n",
    "        self.df[\"batch_id\"] = self.df[\"batch_id\"].astype(int)\n",
    "        self.df[\"id\"] = self.df[\"batch_id\"] * 10 + self.df[\"img_id\"]\n",
    "        self.sequences = []\n",
    "        self.k = sequence_length\n",
    "        self.skip = skip\n",
    "        for _, group in self.df.groupby(\"map\"):\n",
    "            group_sorted = group.sort_values(by=[\"batch_id\", \"img_id\"])\n",
    "            img_ids = group_sorted[\"img_id\"].values\n",
    "            indices = group_sorted.index.values\n",
    "            max_start = len(img_ids) - (self.k - 1) * skip\n",
    "            for i in range(max_start):\n",
    "                seq_indices = indices[i : i + self.k * skip : skip]\n",
    "                if len(seq_indices) == self.k:\n",
    "                    self.sequences.append(\n",
    "                        group[\"img_name\"][seq_indices].values.tolist()\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_names = self.sequences[idx]\n",
    "        images = []\n",
    "        for img_name in img_names:\n",
    "            image = self.process_img(img_name)\n",
    "            images.append(image)\n",
    "        images = torch.stack(images)\n",
    "        return images\n",
    "\n",
    "    def process_img(self, img_name: str):\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e0492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"../datasets/DOOM_yolo_seg/\"\n",
    "dataset = CustomYOLODataset(images_dir=DATASET + \"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f612023",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf8ca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(dataloader))\n",
    "len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66a369d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 3, 640, 640])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5588629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOFeatureExtractor(nn.Module):\n",
    "    def __init__(self, yolo_model, layer_idx=17):\n",
    "        super().__init__()\n",
    "        self.model = yolo_model\n",
    "        self.layer_idx = layer_idx\n",
    "        self.features = None\n",
    "\n",
    "        # Freeze YOLO model parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Register the hook\n",
    "        self.hook = self.model.model.model[self.layer_idx].register_forward_hook(\n",
    "            self._hook_fn\n",
    "        )\n",
    "\n",
    "    def _hook_fn(self, module, input, output):\n",
    "        # Just store the output tensor\n",
    "        self.features = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get batch shape dynamically\n",
    "        batch_size, seq_len, channels, height, width = x.shape\n",
    "\n",
    "        # Clear previous features\n",
    "        self.features = None\n",
    "\n",
    "        # Reshape for YOLO model\n",
    "        x_reshaped = x.view(-1, channels, height, width)\n",
    "\n",
    "        # Run model WITHOUT no_grad() so features can participate in gradient computation\n",
    "        # The YOLO model is frozen via requires_grad=False, so it won't be trained\n",
    "        self.model(x_reshaped, verbose=False)\n",
    "\n",
    "        # Check features were extracted\n",
    "        assert self.features is not None, \"Features not extracted. Check the hook.\"\n",
    "\n",
    "        # Reshape features back to sequence format\n",
    "        features = self.features.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # Return features that can participate in gradient computation\n",
    "        # The YOLO model is frozen, but the features can flow gradients to the LSTM\n",
    "        return features\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"hook\"):\n",
    "            self.hook.remove()\n",
    "\n",
    "\n",
    "# Usage\n",
    "feature_extractor = YOLOFeatureExtractor(model)\n",
    "embeddings = feature_extractor(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a1606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, emb_dim = embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145905eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernEmbeddingLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        bidirectional=False,\n",
    "        use_attention=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate directions\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        # LSTM layer with dropout between layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * self.directions)\n",
    "        # Attention mechanism\n",
    "        if use_attention:\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * self.directions, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "            )\n",
    "        # Output projection to match embedding dimension\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * self.directions, hidden_dim * 2),\n",
    "            nn.GELU(),  # Modern activation function\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, embedding_dim),\n",
    "        )\n",
    "        # Initialize weights for better convergence\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                if \"lstm\" in name:\n",
    "                    nn.init.orthogonal_(param)  # Orthogonal initialization for RNNs\n",
    "                elif len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(\n",
    "                        param\n",
    "                    )  # Xavier for linear layers (2D+ weights)\n",
    "                else:\n",
    "                    nn.init.normal_(\n",
    "                        param, mean=0.0, std=0.02\n",
    "                    )  # Normal init for 1D weights\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)  # Initialize biases to zero\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, embedding_dim]\n",
    "            return_attention: Whether to return attention weights\n",
    "\n",
    "        Returns:\n",
    "            Predicted next embedding of shape [batch_size, embedding_dim]\n",
    "            Optionally, attention weights if return_attention=True\n",
    "        \"\"\"\n",
    "        # x shape: [B, T, embedding_dim]\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "\n",
    "        # Run through LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out shape: [B, T, hidden_dim * directions]\n",
    "\n",
    "        # Apply layer normalization\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        if self.use_attention:\n",
    "            # Calculate attention scores\n",
    "            attention_scores = self.attention(lstm_out).squeeze(-1)  # [B, T]\n",
    "            attention_weights = torch.softmax(attention_scores, dim=1)  # [B, T]\n",
    "\n",
    "            # Apply attention to get context vector\n",
    "            attention_weights = attention_weights.unsqueeze(-1)  # [B, T, 1]\n",
    "            context = torch.sum(\n",
    "                attention_weights * lstm_out, dim=1\n",
    "            )  # [B, hidden_dim * directions]\n",
    "        else:\n",
    "            # Just use the last output\n",
    "            context = lstm_out[:, -1]  # [B, hidden_dim * directions]\n",
    "\n",
    "        # Project to embedding dimension\n",
    "        next_emb = self.proj(context)  # [B, embedding_dim]\n",
    "\n",
    "        if return_attention and self.use_attention:\n",
    "            return next_emb, attention_weights.squeeze(-1)\n",
    "        return next_emb\n",
    "\n",
    "    def predict_sequence(self, initial_sequence, steps=5):\n",
    "        \"\"\"\n",
    "        Generate a sequence of future embeddings\n",
    "\n",
    "        Args:\n",
    "            initial_sequence: Initial sequence of embeddings [B, T, embedding_dim]\n",
    "            steps: Number of future steps to predict\n",
    "\n",
    "        Returns:\n",
    "            Sequence of predicted embeddings [B, steps, embedding_dim]\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = initial_sequence.shape[0]\n",
    "        emb_dim = initial_sequence.shape[2]\n",
    "\n",
    "        # Start with the initial sequence\n",
    "        current_seq = initial_sequence\n",
    "        predictions = torch.zeros(batch_size, steps, emb_dim, device=device)\n",
    "\n",
    "        # Autoregressive prediction\n",
    "        for i in range(steps):\n",
    "            # Predict next embedding\n",
    "            next_emb = self(current_seq)\n",
    "            predictions[:, i] = next_emb\n",
    "\n",
    "            # Update sequence for next prediction (drop oldest, add newest)\n",
    "            if i < steps - 1:  # No need to update for the last step\n",
    "                current_seq = torch.cat(\n",
    "                    [current_seq[:, 1:], next_emb.unsqueeze(1)], dim=1\n",
    "                )\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d5f92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernEmbeddingLSTM(\n",
       "  (lstm): LSTM(102400, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (proj): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=102400, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "lstm_model = ModernEmbeddingLSTM(\n",
    "    embedding_dim=emb_dim, hidden_dim=512, num_layers=2, dropout=0.1, use_attention=True\n",
    ")\n",
    "lstm_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7447388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next embedding shape: torch.Size([8, 102400])\n",
      "Attention weights shape: torch.Size([8, 5])\n",
      "Future sequence shape: torch.Size([8, 1, 102400])\n"
     ]
    }
   ],
   "source": [
    "# Predict next embedding\n",
    "with torch.inference_mode():\n",
    "    next_embedding = lstm_model(embeddings)\n",
    "    print(f\"Next embedding shape: {next_embedding.shape}\")  # [batch_size, emb_dim]\n",
    "\n",
    "    # With attention weights\n",
    "    next_embedding, attention = lstm_model(embeddings, return_attention=True)\n",
    "    print(f\"Attention weights shape: {attention.shape}\")  # [batch_size, seq_len]\n",
    "\n",
    "    # Generate a sequence of future embeddings\n",
    "    future_sequence = lstm_model.predict_sequence(embeddings, steps=1)\n",
    "    print(\n",
    "        f\"Future sequence shape: {future_sequence.shape}\"\n",
    "    )  # [batch_size, 10, emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8366483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/validation split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define split ratio\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5e4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_lstm(\n",
    "    feature_extractor,\n",
    "    lstm_model,\n",
    "    dataloader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    # Move models to device\n",
    "    feature_extractor.to(device)\n",
    "    lstm_model.to(device)\n",
    "\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        lstm_model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    # Training history\n",
    "    history = {\"train_loss\": []}\n",
    "    best_loss = float(\"inf\")\n",
    "    history = {\"train_loss\": []}\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        lstm_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            # Extract embeddings from the entire sequence\n",
    "            embeddings = feature_extractor(batch)  # [B, T, emb_dim]\n",
    "            # No need for requires_grad_(True) - embeddings are frozen features\n",
    "            embeddings = embeddings.to(device)\n",
    "\n",
    "            # Use first N-1 embeddings to predict the Nth embedding_(True)\n",
    "            input_embeddings = embeddings[\n",
    "                :, :-1, :\n",
    "            ]  # [B, T-1, emb_dim])  # Enable gradients for embeddings\n",
    "            target_embedding = embeddings[\n",
    "                :, -1, :\n",
    "            ]  # [B, emb_dim]1 embeddings to predict the Nth embedding\n",
    "            # Forward pass_dim]\n",
    "            optimizer.zero_grad()\n",
    "            predicted_embedding = lstm_model(input_embeddings)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_embedding, target_embedding)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            # Update metricsoptimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.6f}\"\n",
    "                )\n",
    "\n",
    "        # End of epoch\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "\n",
    "        # Update learning rate# End of epoch\n",
    "        scheduler.step()\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} completed. Avg Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}\"\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": lstm_model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": best_loss,\n",
    "                },\n",
    "                \"best_lstm_model.pt\",\n",
    "            )\n",
    "            print(f\"Model saved with loss: {best_loss:.6f}\")\n",
    "\n",
    "    return lstm_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3372b611",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m      9\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m trained_model, history = \u001b[43mtrain_embedding_lstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_embedding_lstm\u001b[39m\u001b[34m(feature_extractor, lstm_model, dataloader, num_epochs, learning_rate, weight_decay, device)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Forward pass_dim]\u001b[39;00m\n\u001b[32m     43\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m predicted_embedding = \u001b[43mlstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     47\u001b[39m loss = criterion(predicted_embedding, target_embedding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DOOM-RL-speedrun/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DOOM-RL-speedrun/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mModernEmbeddingLSTM.forward\u001b[39m\u001b[34m(self, x, return_attention)\u001b[39m\n\u001b[32m     72\u001b[39m batch_size, seq_len = x.shape[\u001b[32m0\u001b[39m], x.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Run through LSTM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m lstm_out, (hidden, cell) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# lstm_out shape: [B, T, hidden_dim * directions]\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Apply layer normalization\u001b[39;00m\n\u001b[32m     79\u001b[39m lstm_out = \u001b[38;5;28mself\u001b[39m.layer_norm(lstm_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DOOM-RL-speedrun/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DOOM-RL-speedrun/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/DOOM-RL-speedrun/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd."
     ]
    }
   ],
   "source": [
    "# Create the feature extractor with dynamic shape handling\n",
    "feature_extractor = YOLOFeatureExtractor(model, layer_idx=17)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)\n",
    "\n",
    "# Train the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "trained_model, history = train_embedding_lstm(\n",
    "    feature_extractor=feature_extractor,\n",
    "    lstm_model=lstm_model,\n",
    "    dataloader=train_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f91801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
